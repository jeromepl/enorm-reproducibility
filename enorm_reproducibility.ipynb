{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "Sz4IUNcpYoRd",
    "outputId": "5d1594f6-6767-4531-8ffb-156ad779a7b3"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "okQDW7sH72BH"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfXH2w1Vcjme"
   },
   "outputs": [],
   "source": [
    "# Most of the code in the present notebook was taken and adapted from the following tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "def get_data(use_valid=False, num_folds=5, fold=0):\n",
    "    '''\n",
    "    Function used to load the CIFAR10 dataset, along with any preprocessing needed.\n",
    "    use_valid = False - Whether to create a validation set from the original training dataset\n",
    "    num_folds = 5 - Number of cross-validation folds\n",
    "    fold = 0 - Fold index to use as validation data\n",
    "    '''\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # Normalize the image by subtracting by the mean and dividing by the standard deviation for each channel\n",
    "        transforms.Lambda(lambda t: t.view(-1)) # Flatten the image, including channels (necessary for fully-connected NN)\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "    num_examples = len(trainset)\n",
    "\n",
    "    # Split into a training and validation set, given the current fold\n",
    "    if use_valid:\n",
    "        num_valid = int(num_examples // num_folds)\n",
    "        train_idx = list(range(0, num_valid * fold)) + list(range(num_valid * (fold + 1), num_examples))\n",
    "        valid_idx = list(range(num_valid * fold, num_valid * (fold + 1)))\n",
    "    else:\n",
    "        train_idx = list(range(num_examples))\n",
    "\n",
    "    trainsampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, # From the paper\n",
    "                                              sampler=trainsampler, num_workers=2)\n",
    "\n",
    "    if use_valid:\n",
    "        validset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                download=True, transform=transform)\n",
    "        validsampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
    "        validloader = torch.utils.data.DataLoader(validset, batch_size=256,\n",
    "                                                  sampler=validsampler, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "                 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    if use_valid:\n",
    "        return trainloader, validloader, testloader, classes\n",
    "    else:\n",
    "        return trainloader, testloader, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOsxjtAT767V"
   },
   "source": [
    "## Defining the network (Section 6.1 in paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Dv17bhD3GYG"
   },
   "source": [
    "** Quote from the paper, describing the network architecture for the CIFAR10 fully-connected experiment:** \n",
    "> We first experiment with a basic fully-connected architecture that takes as input the flattened image of size 3072. Input data is normalized by subtracting mean and dividing by standard deviation independently for each channel. The first linear layer is of size 3072 × 500. We then consider p layers 500 × 500, p being an architecture parameter for the sake of the analysis. The last classification is of size 500 × 10. The weights are initialized with He’s scheme. We train for 60 epochs using SGD with no momentum, a batch size of 256 and weight decay of 10−3. Cross validation is used to pick an initial learning rate in {0.0005, 0.001, 0.005, 0.01, 0.05, 0.1}. PathSGD, GN and WN are learned as detailed in Section 6.1. All results are the average test accuracies over 5 training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0o_ZQLyN-F4f",
    "outputId": "712458e9-31d5-49b2-88d2-bcf4ea5b9d79"
   },
   "outputs": [],
   "source": [
    "# Can we use CUDA?\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c82qKm5b1mQe"
   },
   "outputs": [],
   "source": [
    "# Define the network\n",
    "def create_net(p, batch_norm=False):\n",
    "    '''\n",
    "    Generate a new network of the given depth.\n",
    "    p is the number of intermediary layers in the network\n",
    "    batch_norm = False, whether to use batchnorm on each linear layer (except the output layer)\n",
    "    '''\n",
    "\n",
    "    # Input layer\n",
    "    if batch_norm:\n",
    "        layers = [nn.Linear(3072, 500, bias=False), nn.BatchNorm1d(500), nn.ReLU()]\n",
    "    else:\n",
    "        layers = [nn.Linear(3072, 500, bias=False), nn.ReLU()]\n",
    "    \n",
    "    # Intermediary layers\n",
    "    for i in range(p):\n",
    "        layers.append(nn.Linear(500, 500, bias=False))\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(500))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "    # Output layer\n",
    "    layers.append(nn.Linear(500, 10, bias=False))\n",
    "\n",
    "    net = nn.Sequential(*layers)\n",
    "    net.to(device)\n",
    "\n",
    "    # Initialize weights with He's initialization\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.kaiming_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    net.apply(init_weights)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_ZKIBB_Fxek"
   },
   "source": [
    "## Equi-Normalization\n",
    "Algorithm 1: Equi Normalization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-dfdPaQF1_U"
   },
   "outputs": [],
   "source": [
    "# Apply Equi-normalization directly to the linear layers in a sequential network\n",
    "# Be careful here, the weight matrices stored in Linear layers have size output_size x input_size, which is the opposite of what seems to be assumed in the algorithm\n",
    "def equi_norm(seq_net, c=1.2, C=1, p=2):\n",
    "    '''\n",
    "    Apply equi-normalization to the linear layers in a sequential neural network\n",
    "    c: scaling factor\n",
    "    C: number of enorm cycles to do (usually 1)\n",
    "    p: norm to use (p-norm)\n",
    "    '''\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        layers = [l for l in seq_net.children() if isinstance(l, nn.Linear)]       \n",
    "        for T in range(C):\n",
    "            for k in range(1, len(layers)):\n",
    "                W_prev = layers[k - 1].weight.data\n",
    "                W_k = layers[k].weight.data\n",
    "                size = W_k.size()[1]\n",
    "\n",
    "                R = torch.norm(W_k, p, dim=0)\n",
    "                L = torch.norm(W_prev, p, dim=1)\n",
    "                temp = torch.sqrt(1/c*torch.div(R,L)) # element wise division\n",
    "                D = torch.diag(temp) # creates diagonal matrix from vector\n",
    "                D_inverse = torch.diag(torch.div(torch.ones(size, dtype=W_k.dtype, device=W_k.device),temp))\n",
    "                \n",
    "                # Update the weights\n",
    "                layers[k - 1].weight.data = torch.mm(D, W_prev) # Flip the operation compared to the Algorithm in the paper (see above)\n",
    "                layers[k].weight.data = torch.mm(W_k, D_inverse)\n",
    "                \n",
    "                # Apply the bias normalization from section 3.5 of the paper\n",
    "                if layers[k - 1].bias is not None:\n",
    "                    layers[k - 1].bias.data = torch.mv(D, layers[k - 1].bias.data)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvSw1B0R7_ZV"
   },
   "source": [
    "## Training\n",
    "Algorithm 2: Training with Equi Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67tEiGn16xUG"
   },
   "outputs": [],
   "source": [
    "# Train the network\n",
    "EPOCHS = 60\n",
    "def train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=None, validloader=None, use_enorm=True):\n",
    "    print('Starting training')\n",
    "    \n",
    "    top_valid_acc = -np.inf\n",
    "    top_model_weights = None\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        net.train()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        since = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Use GPU if possible\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ENorm cycle\n",
    "            if use_enorm:\n",
    "                equi_norm(net, c=1.2)\n",
    "\n",
    "            # TODO: If using SGD with momentum, we would need to update the momentum buffers here using the D_k matrices from Algorithm 1\n",
    "            # However, for the experiments that we ran, this was not necessary and was thus not implemented\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # Save network with best validation performance\n",
    "        if validloader is not None:\n",
    "            valid_acc = test(net, validloader)\n",
    "            if valid_acc > top_valid_acc:\n",
    "                top_valid_acc = valid_acc\n",
    "                top_model_weights = copy.deepcopy(net.state_dict())\n",
    "\n",
    "        # print statistics\n",
    "    #     if (epoch + 1) % 10 == 0:\n",
    "        time_elapsed = time.time() - since\n",
    "        if validloader is not None:\n",
    "            print(f\"Finished epoch {epoch + 1} / {EPOCHS}. Loss: {running_loss}. Validation accuracy: {valid_acc}. Time taken: {round(time_elapsed // 60)}m {round(time_elapsed % 60)}s\")\n",
    "        elif testloader is not None:\n",
    "            print(f\"Finished epoch {epoch + 1} / {EPOCHS}. Loss: {running_loss}. Test accuracy: {test(net, testloader)}. Time taken: {round(time_elapsed // 60)}m {round(time_elapsed % 60)}s\")\n",
    "        else:\n",
    "            print(f\"Finished epoch {epoch + 1} / {EPOCHS}. Loss: {running_loss}. Time taken: {round(time_elapsed // 60)}m {round(time_elapsed % 60)}s\")\n",
    "    \n",
    "    if validloader is not None:\n",
    "        print(f'Finished Training. Top validation accuracy: {top_valid_acc}')\n",
    "    else:\n",
    "        print('Finished Training.')\n",
    "    \n",
    "    if top_model_weights is not None:\n",
    "        net.load_state_dict(top_model_weights)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDoTz1HSnKoL"
   },
   "outputs": [],
   "source": [
    "# Define a learning rate scheduler that linearly decays the learning rate from an initial value and a decay/epoch value\n",
    "class LinearLRDecay(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, decay, last_epoch=-1):\n",
    "        self.decay = decay\n",
    "        super(LinearLRDecay, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return [base_lr - self.last_epoch * self.decay for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWYvcJ9W8CDm"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMkPdiMC7GNt"
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy on a dataset\n",
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IvO31tmOc_N"
   },
   "source": [
    "## Running it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1139
    },
    "colab_type": "code",
    "id": "mmSWJhxzOf5d",
    "outputId": "59371208-1bbe-4b56-c20b-82e4925c7d36"
   },
   "outputs": [],
   "source": [
    "trainloader, testloader, classes = get_data()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Run the baseline\n",
    "optimal_baseline_lrs = { 1: 0.1, 3: 0.1, 5: 0.1, 7: 0.1, 9: 0.05, 11: 0.05, 13: 0.05, 15: 0.05, 17: 0.05, 19: 0.05 } # All learning rate values were given to us by the authors\n",
    "for p in range(1, 20, 2):\n",
    "    lr = optimal_baseline_lrs[p]\n",
    "    print(f\"Running baseline w/ p={p}\")\n",
    "    net = create_net(p)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    lr_scheduler = LinearLRDecay(optimizer, lr / EPOCHS)\n",
    "    net = train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=testloader, use_enorm=False) # Baseline for now\n",
    "    test_acc = test(net, testloader)\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {test_acc} %')\n",
    "\n",
    "# Run with ENorm\n",
    "optimal_lrs = { 1: 0.05, 3: 0.05, 5: 0.05, 7: 0.05, 9: 0.05, 11: 0.05, 13: 0.05, 15: 0.05, 17: 0.01, 19: 0.01 }\n",
    "for p in range(1, 20, 2):\n",
    "    print(f\"Running ENorm with p = {p}\")\n",
    "    net = create_net(p)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=optimal_lrs[p], weight_decay=1e-3)\n",
    "    lr_scheduler = LinearLRDecay(optimizer, optimal_lrs[p] / EPOCHS)\n",
    "    net = train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=testloader, use_enorm=True)\n",
    "    test_acc = test(net, testloader)\n",
    "    \n",
    "    print(f'Accuracy of the network on the 10000 test images: {test_acc} %')\n",
    "    \n",
    "# Run a baseline with Batch Norm\n",
    "optimal_bn_lr = 0.1\n",
    "for p in range(1, 20, 2):\n",
    "    print(f\"Running Batch Norm baseline with p = {p}\")\n",
    "    net = create_net(p, batch_norm=True)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=optimal_bn_lr, weight_decay=1e-3)\n",
    "    lr_scheduler = LinearLRDecay(optimizer, optimal_bn_lr / EPOCHS)\n",
    "    net = train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=testloader, use_enorm=False)\n",
    "    test_acc = test(net, testloader)\n",
    "    \n",
    "    print(f'Accuracy of the network on the 10000 test images: {test_acc} %')\n",
    "\n",
    "# Run with Enorm & Batch Norm\n",
    "optimal_en_bn_lrs = { 1: 0.1, 3: 0.1, 5: 0.1, 7: 0.1, 9: 0.1, 11: 0.1, 13: 0.1, 15: 0.05, 17: 0.05, 19: 0.05 } # The last 3 learning rate values here we different than what was given to us by the authors and were found to give better results\n",
    "for p in range(1, 20, 2):\n",
    "    print(f\"Running Batch Norm + ENorm with p = {p}\")\n",
    "    net = create_net(p, batch_norm=True)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=optimal_en_bn_lrs[p], weight_decay=1e-3)\n",
    "    lr_scheduler = LinearLRDecay(optimizer, optimal_bn_lr / EPOCHS)\n",
    "    net = train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=testloader, use_enorm=True)\n",
    "    test_acc = test(net, testloader)\n",
    "    \n",
    "    print(f'Accuracy of the network on the 10000 test images: {test_acc} %')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final-project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
