{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Sz4IUNcpYoRd",
        "colab_type": "code",
        "outputId": "5d1594f6-6767-4531-8ffb-156ad779a7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 27kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x610c4000 @  0x7f14ed2c72a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.9MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-1.0.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "okQDW7sH72BH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ]
    },
    {
      "metadata": {
        "id": "SfXH2w1Vcjme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Most of the code in the present notebook was taken and adapted from the following tutorial: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "def get_data(use_valid=False, num_folds=5, fold=0):\n",
        "    '''\n",
        "    Function used to load the CIFAR10 dataset, along with any preprocessing needed.\n",
        "    use_valid = False - Whether to create a validation set from the original training dataset\n",
        "    num_folds = 5 - Number of cross-validation folds\n",
        "    fold = 0 - Fold index to use as validation data\n",
        "    '''\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), # Normalize the image by subtracting by the mean and dividing by the standard deviation for each channel\n",
        "        transforms.Lambda(lambda t: t.view(-1)) # Flatten the image, including channels (necessary for fully-connected NN)\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    num_examples = len(trainset)\n",
        "\n",
        "    # Split into a training and validation set, given the current fold\n",
        "    if use_valid:\n",
        "        num_valid = int(num_examples // num_folds)\n",
        "        train_idx = list(range(0, num_valid * fold)) + list(range(num_valid * (fold + 1), num_examples))\n",
        "        valid_idx = list(range(num_valid * fold, num_valid * (fold + 1)))\n",
        "    else:\n",
        "        train_idx = list(range(num_examples))\n",
        "\n",
        "    trainsampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, # From the paper\n",
        "                                              sampler=trainsampler, num_workers=2)\n",
        "\n",
        "    if use_valid:\n",
        "        validset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                download=True, transform=transform)\n",
        "        validsampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
        "        validloader = torch.utils.data.DataLoader(validset, batch_size=256,\n",
        "                                                  sampler=validsampler, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "                 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    \n",
        "    if use_valid:\n",
        "        return trainloader, validloader, testloader, classes\n",
        "    else:\n",
        "        return trainloader, testloader, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YOsxjtAT767V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the network (Section 6.1 in paper)"
      ]
    },
    {
      "metadata": {
        "id": "-Dv17bhD3GYG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Quote from the paper, describing the network architecture for the CIFAR10 fully-connected experiment:** \n",
        "> We first experiment with a basic fully-connected architecture that takes as input the flattened image of size 3072. Input data is normalized by subtracting mean and dividing by standard deviation independently for each channel. The first linear layer is of size 3072 × 500. We then consider p layers 500 × 500, p being an architecture parameter for the sake of the analysis. The last classification is of size 500 × 10. The weights are initialized with He’s scheme. We train for 60 epochs using SGD with no momentum, a batch size of 256 and weight decay of 10−3. Cross validation is used to pick an initial learning rate in {0.0005, 0.001, 0.005, 0.01, 0.05, 0.1}. PathSGD, GN and WN are learned as detailed in Section 6.1. All results are the average test accuracies over 5 training runs."
      ]
    },
    {
      "metadata": {
        "id": "0o_ZQLyN-F4f",
        "colab_type": "code",
        "outputId": "712458e9-31d5-49b2-88d2-bcf4ea5b9d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Can we use CUDA?\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {device}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c82qKm5b1mQe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the network\n",
        "def create_net(p, batch_norm=False):\n",
        "    '''\n",
        "    Generate a new network of the given depth.\n",
        "    p is the number of intermediary layers in the network\n",
        "    batch_norm = False, whether to use batchnorm on each linear layer (except the output layer)\n",
        "    '''\n",
        "\n",
        "    # Input layer\n",
        "    if batch_norm:\n",
        "        layers = [nn.Linear(3072, 500, bias=False), nn.BatchNorm1d(500), nn.ReLU()]\n",
        "    else:\n",
        "        layers = [nn.Linear(3072, 500, bias=False), nn.ReLU()]\n",
        "    \n",
        "    # Intermediary layers\n",
        "    for i in range(p):\n",
        "        layers.append(nn.Linear(500, 500, bias=False))\n",
        "        if batch_norm:\n",
        "            layers.append(nn.BatchNorm1d(500))\n",
        "        layers.append(nn.ReLU())\n",
        "        \n",
        "    # Output layer\n",
        "    layers.append(nn.Linear(500, 10, bias=False))\n",
        "\n",
        "    net = nn.Sequential(*layers)\n",
        "    net.to(device)\n",
        "\n",
        "    # Initialize weights with He's initialization\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.kaiming_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0)\n",
        "\n",
        "    net.apply(init_weights)\n",
        "    \n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_ZKIBB_Fxek",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Equi-Normalization\n",
        "Algorithm 1: Equi Normalization Code"
      ]
    },
    {
      "metadata": {
        "id": "R-dfdPaQF1_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Apply Equi-normalization directly to the linear layers in a sequential network\n",
        "# Be careful here, the weight matrices stored in Linear layers have size output_size x input_size, which is the opposite of what seems to be assumed in the algorithm\n",
        "def equi_norm(seq_net, c=1.2, C=1, p=2):\n",
        "    '''\n",
        "    Apply equi-normalization to the linear layers in a sequential neural network\n",
        "    c: scaling factor\n",
        "    C: number of enorm cycles to do (usually 1)\n",
        "    p: norm to use (p-norm)\n",
        "    '''\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        layers = [l for l in seq_net.children() if isinstance(l, nn.Linear)]       \n",
        "        for T in range(C):\n",
        "            for k in range(1, len(layers)):\n",
        "                W_prev = layers[k - 1].weight.data\n",
        "                W_k = layers[k].weight.data\n",
        "                size = W_k.size()[1]\n",
        "\n",
        "                R = torch.norm(W_k, p, dim=0)\n",
        "                L = torch.norm(W_prev, p, dim=1)\n",
        "                temp = torch.sqrt(1/c*torch.div(R,L)) # element wise division\n",
        "                D = torch.diag(temp) # creates diagonal matrix from vector\n",
        "                D_inverse = torch.diag(torch.div(torch.ones(size, dtype=W_k.dtype, device=W_k.device),temp))\n",
        "                \n",
        "                # Update the weights\n",
        "                layers[k - 1].weight.data = torch.mm(D, W_prev) # Flip the operation compared to the Algorithm in the paper (see above)\n",
        "                layers[k].weight.data = torch.mm(W_k, D_inverse)\n",
        "                \n",
        "                # Apply the bias normalization from section 3.5 of the paper\n",
        "                if layers[k - 1].bias is not None:\n",
        "                    layers[k - 1].bias.data = torch.mv(D, layers[k - 1].bias.data)\n",
        "                \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvSw1B0R7_ZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "Algorithm 2: Training with Equi Normalization"
      ]
    },
    {
      "metadata": {
        "id": "67tEiGn16xUG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train the network\n",
        "EPOCHS = 60\n",
        "def train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=None, validloader=None, use_enorm=True):\n",
        "    print('Starting training')\n",
        "    \n",
        "    top_valid_acc = -np.inf\n",
        "    top_model_weights = None\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        \n",
        "        net.train()\n",
        "        \n",
        "        lr_scheduler.step()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        since = time.time()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "\n",
        "            # Use GPU if possible\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # ENorm cycle\n",
        "            if use_enorm:\n",
        "                equi_norm(net, c=1.2)\n",
        "\n",
        "            # TODO: If using SGD with momentum, we would need to update the momentum buffers here using the D_k matrices from Algorithm 1\n",
        "            # However, for the experiments that we ran, this was not necessary and was thus not implemented\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "        # Save network with best validation performance\n",
        "        if validloader is not None:\n",
        "            valid_acc = test(net, validloader)\n",
        "            if valid_acc > top_valid_acc:\n",
        "                top_valid_acc = valid_acc\n",
        "                top_model_weights = copy.deepcopy(net.state_dict())\n",
        "\n",
        "        # print statistics\n",
        "    #     if (epoch + 1) % 10 == 0:\n",
        "        time_elapsed = time.time() - since\n",
        "        if validloader is not None:\n",
        "            print(f\"Finished epoch {epoch + 1} / {EPOCHS}. Loss: {running_loss}. Validation accuracy: {valid_acc}. Time taken: {round(time_elapsed // 60)}m {round(time_elapsed % 60)}s\")\n",
        "        elif testloader is not None:\n",
        "            print(f\"Finished epoch {epoch + 1} / {EPOCHS}. Loss: {running_loss}. Test accuracy: {test(net, testloader)}. Time taken: {round(time_elapsed // 60)}m {round(time_elapsed % 60)}s\")\n",
        "        else:\n",
        "            print(f\"Finished epoch {epoch + 1} / {EPOCHS}. Loss: {running_loss}. Time taken: {round(time_elapsed // 60)}m {round(time_elapsed % 60)}s\")\n",
        "    \n",
        "    if validloader is not None:\n",
        "        print(f'Finished Training. Top validation accuracy: {top_valid_acc}')\n",
        "    else:\n",
        "        print('Finished Training.')\n",
        "    \n",
        "    if top_model_weights is not None:\n",
        "        net.load_state_dict(top_model_weights)\n",
        "    \n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TDoTz1HSnKoL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define a learning rate scheduler that linearly decays the learning rate from an initial value and a decay/epoch value\n",
        "class LinearLRDecay(optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, decay, last_epoch=-1):\n",
        "        self.decay = decay\n",
        "        super(LinearLRDecay, self).__init__(optimizer, last_epoch)\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return [base_lr - self.last_epoch * self.decay for base_lr in self.base_lrs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OWYvcJ9W8CDm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ]
    },
    {
      "metadata": {
        "id": "bMkPdiMC7GNt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Evaluate accuracy on a dataset\n",
        "def test(net, testloader):\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "    return 100.0 * correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8IvO31tmOc_N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Running it all"
      ]
    },
    {
      "metadata": {
        "id": "mmSWJhxzOf5d",
        "colab_type": "code",
        "outputId": "59371208-1bbe-4b56-c20b-82e4925c7d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        }
      },
      "cell_type": "code",
      "source": [
        "trainloader, testloader, classes = get_data()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Run the baseline\n",
        "optimal_baseline_lrs = { 1: 0.1, 3: 0.1, 5: 0.1, 7: 0.1, 9: 0.05, 11: 0.05, 13: 0.05, 15: 0.05, 17: 0.05, 19: 0.05 } # All learning rate values were given to us by the authors\n",
        "for p in range(1, 20, 2):\n",
        "    lr = optimal_baseline_lrs[p]\n",
        "    print(f\"Running baseline w/ p={p}\")\n",
        "    net = create_net(p)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=1e-3)\n",
        "    lr_scheduler = LinearLRDecay(optimizer, lr / EPOCHS)\n",
        "    net = train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=testloader, use_enorm=False) # Baseline for now\n",
        "    test_acc = test(net, testloader)\n",
        "\n",
        "    print(f'Accuracy of the network on the 10000 test images: {test_acc} %')\n",
        "\n",
        "# Run with ENorm\n",
        "optimal_lrs = { 1: 0.05, 3: 0.05, 5: 0.05, 7: 0.05, 9: 0.05, 11: 0.05, 13: 0.05, 15: 0.05, 17: 0.01, 19: 0.01 }\n",
        "for p in range(1, 20, 2):\n",
        "    print(f\"Running ENorm with p = {p}\")\n",
        "    net = create_net(p)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=optimal_lrs[p], weight_decay=1e-3)\n",
        "    lr_scheduler = LinearLRDecay(optimizer, optimal_lrs[p] / EPOCHS)\n",
        "    net = train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=testloader, use_enorm=True)\n",
        "    test_acc = test(net, testloader)\n",
        "    \n",
        "    print(f'Accuracy of the network on the 10000 test images: {test_acc} %')\n",
        "    \n",
        "# Run a baseline with Batch Norm\n",
        "optimal_bn_lr = 0.1\n",
        "for p in range(1, 20, 2):\n",
        "    print(f\"Running Batch Norm baseline with p = {p}\")\n",
        "    net = create_net(p, batch_norm=True)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=optimal_bn_lr, weight_decay=1e-3)\n",
        "    lr_scheduler = LinearLRDecay(optimizer, optimal_bn_lr / EPOCHS)\n",
        "    net = train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=testloader, use_enorm=False)\n",
        "    test_acc = test(net, testloader)\n",
        "    \n",
        "    print(f'Accuracy of the network on the 10000 test images: {test_acc} %')\n",
        "\n",
        "# Run with Enorm & Batch Norm\n",
        "optimal_en_bn_lrs = { 1: 0.1, 3: 0.1, 5: 0.1, 7: 0.1, 9: 0.1, 11: 0.1, 13: 0.1, 15: 0.05, 17: 0.05, 19: 0.05 } # The last 3 learning rate values here we different than what was given to us by the authors and were found to give better results\n",
        "for p in range(1, 20, 2):\n",
        "    print(f\"Running Batch Norm + ENorm with p = {p}\")\n",
        "    net = create_net(p, batch_norm=True)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=optimal_en_bn_lrs[p], weight_decay=1e-3)\n",
        "    lr_scheduler = LinearLRDecay(optimizer, optimal_bn_lr / EPOCHS)\n",
        "    net = train(net, criterion, optimizer, lr_scheduler, trainloader, testloader=testloader, use_enorm=True)\n",
        "    test_acc = test(net, testloader)\n",
        "    \n",
        "    print(f'Accuracy of the network on the 10000 test images: {test_acc} %')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Running Batch Norm + ENorm with p = 19\n",
            "Starting training\n",
            "Finished epoch 1 / 60. Loss: 768.2842626571655. Test accuracy: 18.27. Time taken: 0m 16s\n",
            "Finished epoch 2 / 60. Loss: 496.2896775007248. Test accuracy: 23.76. Time taken: 0m 16s\n",
            "Finished epoch 3 / 60. Loss: 424.3845908641815. Test accuracy: 29.28. Time taken: 0m 16s\n",
            "Finished epoch 4 / 60. Loss: 385.5180605649948. Test accuracy: 28.3. Time taken: 0m 16s\n",
            "Finished epoch 5 / 60. Loss: 362.17476403713226. Test accuracy: 33.36. Time taken: 0m 16s\n",
            "Finished epoch 6 / 60. Loss: 341.69117164611816. Test accuracy: 21.92. Time taken: 0m 16s\n",
            "Finished epoch 7 / 60. Loss: 365.1970913410187. Test accuracy: 37.93. Time taken: 0m 16s\n",
            "Finished epoch 8 / 60. Loss: 348.1666656732559. Test accuracy: 19.13. Time taken: 0m 16s\n",
            "Finished epoch 9 / 60. Loss: 374.2882846593857. Test accuracy: 34.64. Time taken: 0m 16s\n",
            "Finished epoch 10 / 60. Loss: 346.8195116519928. Test accuracy: 38.62. Time taken: 0m 16s\n",
            "Finished epoch 11 / 60. Loss: 329.3322044610977. Test accuracy: 37.85. Time taken: 0m 16s\n",
            "Finished epoch 12 / 60. Loss: 318.8838632106781. Test accuracy: 36.67. Time taken: 0m 16s\n",
            "Finished epoch 13 / 60. Loss: 307.4127560853958. Test accuracy: 45.14. Time taken: 0m 16s\n",
            "Finished epoch 14 / 60. Loss: 302.7648286819458. Test accuracy: 37.55. Time taken: 0m 16s\n",
            "Finished epoch 15 / 60. Loss: 291.3698947429657. Test accuracy: 46.21. Time taken: 0m 16s\n",
            "Finished epoch 16 / 60. Loss: 284.1472270488739. Test accuracy: 25.66. Time taken: 0m 16s\n",
            "Finished epoch 17 / 60. Loss: 290.7693885564804. Test accuracy: 45.06. Time taken: 0m 16s\n",
            "Finished epoch 18 / 60. Loss: 276.78714621067047. Test accuracy: 36.4. Time taken: 0m 16s\n",
            "Finished epoch 19 / 60. Loss: 265.0205000638962. Test accuracy: 45.76. Time taken: 0m 16s\n",
            "Finished epoch 20 / 60. Loss: 257.4249621629715. Test accuracy: 48.42. Time taken: 0m 16s\n",
            "Finished epoch 21 / 60. Loss: 246.70203757286072. Test accuracy: 46.37. Time taken: 0m 16s\n",
            "Finished epoch 22 / 60. Loss: 239.58331990242004. Test accuracy: 45.93. Time taken: 0m 16s\n",
            "Finished epoch 23 / 60. Loss: 232.95091795921326. Test accuracy: 47.16. Time taken: 0m 16s\n",
            "Finished epoch 24 / 60. Loss: 226.73080831766129. Test accuracy: 38.57. Time taken: 0m 16s\n",
            "Finished epoch 25 / 60. Loss: 223.36813014745712. Test accuracy: 48.37. Time taken: 0m 16s\n",
            "Finished epoch 26 / 60. Loss: 214.51370590925217. Test accuracy: 47.72. Time taken: 0m 16s\n",
            "Finished epoch 27 / 60. Loss: 208.50523203611374. Test accuracy: 47.62. Time taken: 0m 16s\n",
            "Finished epoch 28 / 60. Loss: 202.5287525653839. Test accuracy: 48.09. Time taken: 0m 16s\n",
            "Finished epoch 29 / 60. Loss: 196.0892106294632. Test accuracy: 50.49. Time taken: 0m 16s\n",
            "Finished epoch 30 / 60. Loss: 190.03381824493408. Test accuracy: 49.83. Time taken: 0m 16s\n",
            "Finished epoch 31 / 60. Loss: 185.23392736911774. Test accuracy: 48.63. Time taken: 0m 16s\n",
            "Finished epoch 32 / 60. Loss: 179.2350773215294. Test accuracy: 44.57. Time taken: 0m 16s\n",
            "Finished epoch 33 / 60. Loss: 173.67624825239182. Test accuracy: 48.98. Time taken: 0m 16s\n",
            "Finished epoch 34 / 60. Loss: 167.5956056714058. Test accuracy: 48.63. Time taken: 0m 16s\n",
            "Finished epoch 35 / 60. Loss: 163.34701091051102. Test accuracy: 47.62. Time taken: 0m 16s\n",
            "Finished epoch 36 / 60. Loss: 157.2102748155594. Test accuracy: 49.23. Time taken: 0m 16s\n",
            "Finished epoch 37 / 60. Loss: 152.1503511071205. Test accuracy: 40.27. Time taken: 0m 16s\n",
            "Finished epoch 38 / 60. Loss: 148.11093544960022. Test accuracy: 50.53. Time taken: 0m 16s\n",
            "Finished epoch 39 / 60. Loss: 139.40655851364136. Test accuracy: 49.73. Time taken: 0m 16s\n",
            "Finished epoch 40 / 60. Loss: 133.86858439445496. Test accuracy: 44.43. Time taken: 0m 16s\n",
            "Finished epoch 41 / 60. Loss: 129.5489077270031. Test accuracy: 46.24. Time taken: 0m 16s\n",
            "Finished epoch 42 / 60. Loss: 124.48795953392982. Test accuracy: 49.24. Time taken: 0m 16s\n",
            "Finished epoch 43 / 60. Loss: 119.91630226373672. Test accuracy: 49.59. Time taken: 0m 16s\n",
            "Finished epoch 44 / 60. Loss: 111.42703592777252. Test accuracy: 49.45. Time taken: 0m 16s\n",
            "Finished epoch 45 / 60. Loss: 109.53355595469475. Test accuracy: 50.36. Time taken: 0m 16s\n",
            "Finished epoch 46 / 60. Loss: 99.96500325202942. Test accuracy: 50.63. Time taken: 0m 16s\n",
            "Finished epoch 47 / 60. Loss: 95.9680908024311. Test accuracy: 49.1. Time taken: 0m 16s\n",
            "Finished epoch 48 / 60. Loss: 90.75619572401047. Test accuracy: 46.88. Time taken: 0m 16s\n",
            "Finished epoch 49 / 60. Loss: 86.65525951981544. Test accuracy: 49.63. Time taken: 0m 16s\n",
            "Finished epoch 50 / 60. Loss: 78.60520139336586. Test accuracy: 51.97. Time taken: 0m 16s\n",
            "Finished epoch 51 / 60. Loss: 73.26331727206707. Test accuracy: 51.82. Time taken: 0m 16s\n",
            "Finished epoch 52 / 60. Loss: 67.91978159546852. Test accuracy: 48.49. Time taken: 0m 16s\n",
            "Finished epoch 53 / 60. Loss: 64.24345915019512. Test accuracy: 52.21. Time taken: 0m 16s\n",
            "Finished epoch 54 / 60. Loss: 56.516949251294136. Test accuracy: 51.57. Time taken: 0m 16s\n",
            "Finished epoch 55 / 60. Loss: 52.53309440612793. Test accuracy: 52.27. Time taken: 0m 16s\n",
            "Finished epoch 56 / 60. Loss: 49.16014455258846. Test accuracy: 51.92. Time taken: 0m 16s\n",
            "Finished epoch 57 / 60. Loss: 44.323827266693115. Test accuracy: 52.26. Time taken: 0m 16s\n",
            "Finished epoch 58 / 60. Loss: 41.05118454992771. Test accuracy: 52.9. Time taken: 0m 16s\n",
            "Finished epoch 59 / 60. Loss: 37.97042817622423. Test accuracy: 53.13. Time taken: 0m 16s\n",
            "Finished epoch 60 / 60. Loss: 35.72868712991476. Test accuracy: 53.28. Time taken: 0m 16s\n",
            "Finished Training.\n",
            "Accuracy of the network on the 10000 test images: 53.28 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}